Markov Decision Processes (MDPs)
* Decsiion maker (Agent)
* Each timestep agent will get representation of envionment state
* Agent select an action to take
* Agent given a reward of its previos action

---
# Components of a MDP
* Environment
* Agent 
* Possible **States** of Environment (S)
* Possible **Actions** an agent can take (A)
* **Rewards** (R)

 Sequence of States Actions and rewards is called a trajectory

---
Agent wants to get the best reward
---
# Expected Returns
expected return is the sum of future rewards


Epesode = a biginning and end of a tasks
Eposidic task - Tasks that are happening in an epesode

continuous - no episode -- complex tasks

---

Discount if continous use a discounted return.  The agent values more imediate rewards rather than future rewards.



