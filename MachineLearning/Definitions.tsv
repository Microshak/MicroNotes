Overfit      corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably"
Underfit     occurs when a statistical model cannot adequately capture the underlying structure of the data    
feedforward neural  network     is an artificial neural network wherein connections between the nodes do not form a cycle.
Activation function  nonlinear(cruved) maping to layers It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function)
Diferationable funciton  that we can find its dirivitive
Activation Potential    lets us know weather to activate a neuron or not
optomizers   Update network weights iterative based in training data
Bias    is error due to erroneous or overly simplistic assumptions in the learning algorithm you’re using  This can lead to the model underfitting your data, making it hard for it to have high predictive accuracy and for you to generalize your knowledge from the training set to the test set
Variance     is error due to too much complexity in the learning algorithm you’re using. This leads to the algorithm being highly sensitive to high degrees of variation in your training data, which can lead your model to overfit the data. You’ll be carrying too much noise from your training data for your model to be very useful for your test data
ROC Curve     Reciever Operating characteristic graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).
Recall   is also known as the true positive rate: the amount of positives your model claims compared to the actual number of positives there are throughout the data.
Precision    also known as the positive predictive value, and it is a measure of the amount of accurate positives your model claims compared to the number of positives it actually claim
Naive Bayes     is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life.
Type I error    is a false positive
Type II error     is a false negative
F1 score       is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst
Steps    to make an estimator  train - evaluate - predict - export model
Eager execution      imperative, define-by-run interface for advanced operations
Estimator    Estimators can train large models on multiple machines 