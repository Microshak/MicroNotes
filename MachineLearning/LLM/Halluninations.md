
Hallucinations in Large Language Models stem from data compression and inconsistency.

* Adjust the temperature parameter to limit model creativity.
* Pay attention to prompt engineering. Ask the model to think step-by-step and provide facts and references to sources in the response.
* Incorporate external knowledge sources for improved answer verification.

# Hallunination Types
* Logical fallicy - Reasoning mistakes
* Fabrication of Facts
* Data=driven Bias - training data is bias.

# benchmarks for evaluating hallucination

* Knowledge-oriented LLM Assessment benchmark (KoLA)
* TruthfulQA: Measuring How Models Mimic Human Falsehoods
* Medical Domain Hallucination Test for Large Language Models
* HaluEval: A Hallucination Evaluation Benchmark for LLMs

# Temperature
* A modelâ€™s temperature refers to a scalar value used to adjust the probability distribution predicted by the model
*  Balances between sticking to what the model has learned from the training data and generating more diverse or creative responses
*  Creative answers are more prone to hallucinations.
