![full](https://miro.medium.com/v2/resize:fit:720/format:webp/1*VLQZBgG9K-P8lMBAjVpbeg.png)

---

# Generative AI Lifecycle
![full](https://miro.medium.com/v2/resize:fit:720/format:webp/1*VLQZBgG9K-P8lMBAjVpbeg.png)

---

ref
[Attention is all you need](https://arxiv.org/abs/1706.03762)


* Embedding Databases
* LangChain




# Background
1. LLMs Historically RNNs.
2. Attention is All You Need”  The transformer enabling efficient scaling on multi-core GPUs, parallel processing of input data, and harnessing larger training datasets.
3. ability to learn and use attention mechanisms, allowing the model to focus on the meaning of the words being processed
4. Transformers
5. Before words are tokenized into venctors (with positional info) 
  6. Self Attention. Assinging attention waits between relations between words it can understand how language works
  7.  Architecture
    7. Encoder: Processes input sequences passing them through multiple attentio layer
    8. Decoder: utilizes the encoder’s contextual understanding, starting with a start-of-sequence token, to generate new tokens in a loop
    9. Softmax normalization provies a probably score for each word in the     


