
# Background
1. LLMs Historically RNNs.
2. Attention is All You Need”  The transformer enabling efficient scaling on multi-core GPUs, parallel processing of input data, and harnessing larger training datasets.
3. ability to learn and use attention mechanisms, allowing the model to focus on the meaning of the words being processed
4. Transformers
5. Before words are tokenized into venctors (with positional info) 
  6. Self Attention. Assinging attention waits between relations between words it can understand how language works
  7.  Architecture
    7. Encoder: Processes input sequences passing them through multiple attentio layer
    8. Decoder: utilizes the encoder’s contextual understanding, starting with a start-of-sequence token, to generate new tokens in a loop
    9. Softmax normalization provies a probably score for each word in the     


# Temperature
* A model’s temperature refers to a scalar value used to adjust the probability distribution predicted by the model
*  Balances between sticking to what the model has learned from the training data and generating more diverse or creative responses
*  Creative answers are more prone to hallucinations.
---

ref
[Attention is all you need](https://arxiv.org/abs/1706.03762)


* Embedding Databases
* LangChain
