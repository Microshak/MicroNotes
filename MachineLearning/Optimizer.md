# Terms
* backpropagation  -- Backpropagation is a method for training neural networks by adjusting the weights of the connections based on the error between the networkâ€™s output and the desired output
* gradient descent -- Gradient descent is an optimization algorithm that is used to train machine learning models and neural networks by minimizing the error between the predicted and the actual results. It works by iteratively updating the parameters of the model based on the gradient of the cost function, which measures how well the model fits the data
* exponentially moving average -- Exponentially moving average (EMA) is a type of moving average that assigns more weight to the most recent data points than the older ones. It is a way of smoothing out the fluctuations in the data and capturing the long-term trend. EMA is often used in time series analysis, optimization algorithms, and deep learning models1

* Convergence in machine learning means that a model has reached a stable point where further training will not improve its performance


