Sigmiod
Takes some number and squishit bewteen  0 and 1.
Problem:  makes the gradient vanish
    tends to make the gradients either all positives or all negatve

TanH
Suashes everything between -1 and 1
Problem: vanishing gradient


Relu - Rectified Linear Unit
    When <0 then 0
    else linear
Weird but has a 6X improveent
ONLY FOR HIDDEN Layers

Softmax
FOR CLASSIFICATION
Probabilies for different classes
PRoblem:


Linear
FOR REGRESSION


